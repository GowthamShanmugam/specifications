= Pluggable Alerts Delivery End-Points

== Introduction

The alerting module in tendrl needs to alert/notify

* State changes of tendrl managed entities
* Threshold breaches of utilizations of tendrl managed entities.

via mediums such as:

* SNMP-traps
* email

The different sources of alerts are:

* Performance monitoring threshold breach alerts generated by collectd.
* Alerts related to state changes of tendrl managed sds specific entities
  generated by sds-integration.
  ex: Tendrl/gluster-integration, Tendrl/ceph-integration

== Problem description

* The different tendrl supported alerting means need to be pluggable in the
  sense that adding any new alert notifying means should be an easy task and
  should not involve much changes in the core alerting framework.
* The alerting mechanism should have a way to set importance of alerts so that
  the alerts desingated as important will be notified to users even if the
  alerting module crashed and hence the alert could not be notified but will
  be alerted once the alerting module is started back.

== Use Cases

* Dynamic addition of new means of notifying an alert.

== Proposed change

* The alerts raised are persisted in '/alerts' directory of etcd before sending
  out notifications. The flow is as below:
  ** One socket is connected to for writes from collectd, read-only from the
     node agent and read-write from the bridge.
  ** The generator(collectd or bridge) of the alert needs to put the alert on
     this socket.
  ** Any alert on that socket is always taken to etcd by the node agent
  ** Bridge can read the alerts and act on only the ones it can act on, ignore
     the rest it can also generate it's own alerts and put them on the socket
     for node agent to transport to etcd
* Alert meta-data:
  ** Each alert will carry additional attribute that carries its significance
     which will be added by respective handler. The supported significance
     levels can be 'HIGH', 'MEDIUM' and 'LOW'.
    *** The 'LOW' significance level alerts will not be notified. But will
        feature in the list of alerts in etcd.
    *** The 'MEDIUM' significance level alerts will be attempted to be notified
    *** The 'HIGH' significance level alerts will be re-attempted to be
        notified(on restart) if a previous attempt failed for some reason.
        (crash of alerting service).The alerts marked as significance HIGH are
        deemed important and hence their delivery is guranteed even in case of
        sudden crash of alerting module. This is achieved by updating a field
        ('notified': True) of the alert in etcd directory to indicate that the
        alert has been notified. When alerting module is started, it scans all
        alerts in '/alerts' directory and sends out notifications for those
        that are marked significance 'HIGH' but are not set with
        'notified':True
  ** An alert will also have a field to ack it. Acking an alert can be done by
     tendrl-user(lets say user is aware of a problem and has taken some action
     on it and doesn't want to be shown the alert until the issue is completely
     rectified. In such case user can ack such an alert using an api that
     tendrl-api exposes and such an api will set 'acked': True and
     'acked-by': <user_name> to indicate user <user-name> acked the alert and
     doesn't want to see it in the list of alerts) or occurence of negation of
     existing alert(ex: Lets say cpu utilization of a node breached critical
     threshold and a corresponding alert is raised by collectd and the alert is
     stored in etcd by the node-agent after a while when the cpu utilization is
     back normal, previous alert corresponding to its threshold breach needs
     to be acked as no more relevant. This is achieved by setting 'acked' to True
     and 'ackedby' to tendrl which means tendrl has acked this alert as no more
     relevant)
  ** In addition to the above, the alert contains details such as the id of the
     node on which the alert was detected, the time stamp at which the alert
     was observed, resource for which the alert was generated, the type of
     alert, severity of alert etc... as described in section
     "Data model impact".
* The Tendrl/alerting module watches for new alerts in etcd's /alerts directory
  and sends out notifications to configured destinations via configured means
  ex: mails or sms or snmp traps.
  The different notification medium will be hooked into tendrl in a pluggable
  manner as follows:
  ** Handlers will be defined in alerting module for dispatching observed
     alerts to configured destinations through respective means.
     ex: mail handler for sending out mail and snmp handler for snmp-trap
    *** A new handler can be hooked into the tendrl alerting system by adding
        it to a designated directory and with a name prefixed with notification
        medium name. ex: email_handler for mail notification in the directory
        alerting/tendrl/alerting/notification/
    *** Whenever a new handler is added, the first thing the handler is
        supposed to do is add itself as a means to list of supported
        notification mechanisms in etcd in '/notification_medium' directory.
    *** The alerting module will expose apis for managing configurations and
        subscriptions for each of the notification medium which will be proxied
        to by tendrl-api.
        The way the api works is as follows:
        **** The configuration if applicable will link to the tendrl maintained
             users by their id.ex: User->email config linking is possible but
             not User->snmp configuration.
        **** The configuration for each of these handlers will be stored in a
             respective directory in etcd.
             ex: Email configuration will be maintained in a etcd's directory
                 (ex: /notification_medium/email/config). Likewise, snmp
                 configuration will be maintained in etcd's
                 '/notification_medium/snmp/config' directory.
        **** The handler takes the responsibility of performing validation of
             the configuration. Some of the sample validations include:
              ***** Presence of configuration for admin user before any email
                    notification is dispatched to act as source(from) of all
                    mail notifications.
              ***** Presence of all mandatory fields in config mandated by the
                    respective handler.

----
  Note:
    * The different alerts need to be yet classified on various importance
      levels.
      And this is my current opinion:
      **  Any threshold breach corresponding notification is a 'HIGH'
          significance alerts
    * The complete list of alerts and their importance/significance level needs
      to be decided.Following are some of the github issues to capture the list
      ** https://github.com/Tendrl/documentation/issues/44
      ** https://github.com/Tendrl/documentation/issues/45
      ** https://github.com/Tendrl/documentation/issues/46
----


=== Alternatives

To be explored.

=== Data model impact

The structure of alert can be the following:

----
{
  'alert-id': <unique tendrl generated id>,
  'node-id': <id of node on which alert was detected>,
  'time-stamp': <time stamp of alert>,
  'resource': <the name of resource for which alert has been raised>,
  'current-value': <the current observed value status/utilization as applies>
  'tags': <custom alert specific info>,
  'type': <the type of alert percent-used/status of resource>,
  'severity': <severity of alert>,
  'significance': <the severity of importance of notifying the alert>,
  'ackedby': <indicate who acked the alert>,
  'acked': <boolean to indicate if the alert is acked>,
}
----

ex:
For a performance monitoring related alert:

----
{
  alert_id : '6405962e-bc46-11e6-a4a6-cec0c932ce01',
  node_id: '5205962e-bc46-11e6-a4a6-cec0c932cz01',
  time_stamp: '1481046935.536',
  'resource': 'memory',
  'CurrentValue': '7.176942e+00',
  tags: {
    'WarningMax': '1.000000e+00',
    'FailureMax': '2.000000e+00',
  },
  'Type': 'percent-used',
  'severity': 'Critical',
  'ackedby': '',
  'acked': False,
  'significance': 'HIGH',
}
----

and for a status based alert:

----
{
  alert_id : '6405962e-bc46-11e6-a4a6-cec0c932ce01',
  node_id: '5205962e-bc46-11e6-a4a6-cec0c932cz01',
  time_stamp: '1481046935.536',
  'resource': 'cluster',
  'CurrentValue': 'Down',
  tags: {
    'cluster_id' : '6406062e-be46-11e6-a4a6-cec0c932ce01',
  },
  'Type': 'status',
  'severity': 'Critical',
  'ackedby': '',
  'acked': False,
  'significance': 'HIGH',
}
----

The severity levels can be 'Critical', 'Info' or 'Warning'.

=== Impacted Modules:

==== Tendrl API impact:

The tendrl api needs to proxy to apis exposed by Tendrl/alerting as mentioned
in section below(Notifications/Monitoring impact)

==== Notifications/Monitoring impact:

Tendrl/alerting needs to implement flows for apis as described in section
"Tendrl API impact".

The flow definition for the above will look like:

----
# flake8: noqa
data = """---
namespace.tendrl.alerting:
  objects:
    Alert:
      attrs:
        alert-id:
          type: String
        node-id:
          type: String
        time-stamp:
          type: String
        resource:
          type: String
        current-value:
          type: String
        tags:
          type: String
        type:
          type: String
        severity:
          type: String
        significance:
          type: String
        ackedby:
          type: String
        acked:
          type: Boolean
      enabled: true
      value: alerts/$Alert.alert_id
      list: alerts/
        filter_criteria:
          type: json
    NotificationMedia:
      attrs:
        name:
          type: String
        list: alerts/notification_medium/
tendrl_schema_version: 0.3
"""
----

* This adds the following apis:
  ** Api to get list of currently supported means of notification.

----
GET /alerting

Sample Response:

Status: 200 OK
{
  email,
  snmp
}
----

  ** Api to get list of alerts with various filtering options such as based on
     time, acked/not acked, alert type, severity, resource and significance.

----
GET /alerts/severity='CRITICAL'

Sample Response:

Status: 200 OK
{
  'resource': u'swap',
  'severity': u'CRITICAL',
  'tags': {
    'message': u'Host dhcp43-30.lab.eng.blr.redhat.com,plugin swap type percent (instance used): Data source "value" is currently 2.399964. That is above failure threshold of 2.000000.\n',
    'warning_max': u'1.000000e+00',
    'failure_max': u'2.000000e+00'
  },
  'pid': '21688',
  'source': 'collectd',
  'host': u'dhcp43-30.lab.eng.blr.redhat.com',
  'current_value': u'2.399964e+00',
  'time_stamp': u'1481345075.096',
  'type': u'percent'
}
----

  ** API to post configuration
----
Note: API format to be worked through
----

==== Tendrl/common impact:

None

==== Tendrl/node_agent impact:

None

==== Sds integration impact:

None

=== Security impact:

None

=== Other end user impact

None

=== Performance impact

None

=== Other deployer impact

None


=== Developer impact

== Implementation

=== Assignee(s)

Primary assignee:

  * Changes in alerting module : Anmol Babu

== Dependencies:

* User management in tendrl.

== Documentation impact

As described in section "Tendrl API impact" new apis will be added.

== Testing

This spec introduces an api to list available means of alert notification which
needs to be tested.

== References

* Comments on https://github.com/Tendrl/alerting/pull/1
* https://github.com/Tendrl/documentation/issues/44
* https://github.com/Tendrl/documentation/issues/45
* https://github.com/Tendrl/documentation/issues/46
